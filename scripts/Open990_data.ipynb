{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext # https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n",
    "from os.path import join, abspath\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/21138751/spark-java-lang-outofmemoryerror-java-heap-space/22742982#22742982\n",
    "# https://luminousmen.com/post/spark-partitions\n",
    "# https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html\n",
    "warehouse_location = abspath('../data/spark-warehouse')\n",
    "# Create the session\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.ui.port\", \"4041\")\n",
    "    .set('spark.sql.warehouse.dir', warehouse_location)\n",
    "#    .set('spark.executor.memory', '6G')\n",
    "#    .set('spark.driver.memory', '6G')\n",
    "#    .set('spark.storage.memoryFraction', '.5')\n",
    "#    .set('spark.driver.maxResultSize', '2G')\n",
    "#    .set(\"spark.dynamicAllocation.enabled\", \"true\")   \n",
    "#    .set(\"spark.executor.cores\", \"4\")\n",
    "#    .set(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "#    .set(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n",
    "#    .set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "       )\n",
    "\n",
    "# Create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = (SparkSession.builder\n",
    "    .appName('Spark Practice')\n",
    "    .getOrCreate())\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version = 3.0.1\n",
      "Hadoop version = 3.2.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spark version = {spark.version}\")\n",
    "print(f\"Hadoop version = {sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW DATABASES;').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1606608100151'),\n",
       " ('spark.driver.host', '6ef172d4d6e2'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.sql.warehouse.dir', '/home/jovyan/cse451/spark-warehouse/test'),\n",
       " ('spark.ui.port', '4041'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '37455'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56927329/spark-option-inferschema-vs-header-true/56933052\n",
    "df_master = (spark.read.format('csv')\n",
    "    .option('header', 'true') # first row is column names.\n",
    "    .option(\"inferSchema\", \"true\") # set column types       \n",
    "    .load('../data/Open990/IRS_Master_File/*.csv'))\n",
    "\n",
    "df_master = df_master.toDF(*[c.lower() for c in df_master.columns]) # create lowercase columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contractor = (spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load('../data/Open990/Open990_Contractor_Compensation_Snack_Set_Public/Open990_Contractor_Compensation_Snack_Set_Public_2019-01-24.csv'))\n",
    "df_governance = (spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load('../data/Open990/Open990_Governance_Snack_Set_Public/Open990_Governance_Snack_Set_Public_2019-01-15.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_executive_charity = (spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load('../data/Open990/Open990_SnackSet_Executive_Compensation/Compensation_charities_Open990.csv'))\n",
    "df_executive_charity = df_executive_charity.toDF(*[c.lower().replace(' ', \"_\").replace('-', '_') for c in df_executive_charity.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_executive_foundations = (spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load('../data/Open990/Open990_SnackSet_Executive_Compensation/Compensation_foundations_Open990.csv'))\n",
    "df_executive_foundations = df_executive_foundations.toDF(*[c.lower().replace(' ', \"_\").replace('-', '_') for c in df_executive_foundations.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grants = (spark.read.format('csv')\n",
    "    .option('header', 'true')\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"comment\", \"F\")\n",
    "    .load('../data/Open990/Open990_SnackSet_Foundations_Grants/Grants.csv'))\n",
    "df_grants = df_grants.toDF(*[c.lower().replace(' ', \"_\").replace('-', '_') for c in df_grants.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foundations = df_executive = (spark.read.format('csv')\n",
    "    .option('header', 'true', )\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"comment\", \"F\")\n",
    "    .load('../data/Open990/Open990_SnackSet_Foundations_Grants/Foundations.csv'))\n",
    "df_foundations = df_foundations.toDF(*[c.lower().replace(' ', \"_\").replace('-', '_') for c in df_foundations.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123581\n",
      "234492\n",
      "237448\n",
      "913386\n",
      "1453208\n",
      "1757712\n",
      "3990749\n"
     ]
    }
   ],
   "source": [
    "print(df_foundations.count())\n",
    "print(df_executive_foundations.count()) \n",
    "print(df_contractor.count()) \n",
    "print(df_grants.count())\n",
    "print(df_governance.count()) \n",
    "print(df_master.count())\n",
    "print(df_executive_charity.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foundations = df_foundations.repartition(20)\n",
    "df_executive_foundations = df_executive_foundations.repartition(20)\n",
    "df_contractor = df_contractor.repartition(20) \n",
    "df_grants = df_grants.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_governance = df_governance.repartition(200)\n",
    "df_master = df_master.repartition(200)\n",
    "df_executive_charity = df_executive_charity.repartition(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE Open990\")\n",
    "spark.sql(\"USE Open990\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_executive_foundations.columns\n",
    "# df_grants.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "_Need to set the docker desktop memory setting to be bigger than 4 gigs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES IN Open990').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foundations.write.saveAsTable(\"foundations\")\n",
    "df_executive_foundations.write.saveAsTable(\"executive_foundations\")\n",
    "df_contractor.write.saveAsTable(\"contractor\")\n",
    "df_grants.write.saveAsTable(\"grants\")\n",
    "df_governance.write.saveAsTable(\"governance\")\n",
    "df_master.write.saveAsTable(\"master\")\n",
    "df_executive_charity.write.saveAsTable(\"executive_charity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
